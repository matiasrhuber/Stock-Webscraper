{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas_datareader import data\n",
    "import pandas_datareader as web\n",
    "from bs4 import BeautifulSoup\n",
    "import yaml\n",
    "import json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo Finance WEBSCRAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = ['GOOGL','AAPL','TSLA','AMZN','META', 'SNAP']\n",
    "User_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Stock prices\n",
    "base_url = 'https://finance.yahoo.com/quote/'\n",
    "tickr_url = [f'{x}?p={x}&.tsrc=fin-srch' for x in stocks]\n",
    "\n",
    "current_price = []\n",
    "missing_data = []\n",
    "print('Getting current prices for:')\n",
    "for tickr,stock in zip(tickr_url,stocks):\n",
    "    print(f'...{stock}...')\n",
    "    try:\n",
    "        content = requests.get(base_url+tickr,headers={'User-Agent':User_agent}).content\n",
    "        soup = BeautifulSoup(content)\n",
    "        soup = soup.find('fin-streamer',class_='Fw(b) Fz(36px) Mb(-4px) D(ib)') #Fw(b) Fz(36px) Mb(-4px) D(ib)\n",
    "        current_price.append(float(soup['value']))\n",
    "    except:\n",
    "        print(f'No data for {stock}')\n",
    "        missing_data.append(stock)\n",
    "        current_price.append('Na')\n",
    "\n",
    "pd.DataFrame({'Ticker':stocks, 'Current Price':current_price})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20Y Historical Stock Prices\n",
    "base_url = 'https://query1.finance.yahoo.com/v7/finance/download/{}?' # 'AAPL?period1=1645596185&amp;period2=1677132185&amp;interval=1d&amp;events=history&amp;includeAdjustedClose=true'\n",
    "params = {'range': '20y',\n",
    "          'interval': '1d',\n",
    "          'events':'history'}\n",
    "print('Getting ({}) historical prices for:'.format(params['range']))\n",
    "for stock in stocks:\n",
    "    print(f'...{stock}...')\n",
    "    try:\n",
    "        os.mkdir('./Historical/{}/{}'.format(params['range'],stock))\n",
    "        print('New directory completed')\n",
    "    except FileExistsError:\n",
    "        print('A directory already exists, additional data for {} is being saved'.format(stock))\n",
    "    try:\n",
    "        response = requests.get(base_url.format(stock), params=params,headers={'User-Agent': User_agent}).content\n",
    "        soup = BeautifulSoup(response)\n",
    "        ls = str(soup.find('p')).replace('<p>','').splitlines()\n",
    "        cols = ls[0]\n",
    "        data = np.array([x.split(',') for x in ls[1:]])\n",
    "        pd.DataFrame(data=data,columns=cols.split(',')).to_csv('./Historical/{}/{}/historical_price.csv'.format(params['range'],stock))\n",
    "    except:\n",
    "        print(f'No data for {stock}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEC_user_agent = 'matiasrhuber@gmail.com'\n",
    "encoding = 'gzip, deflate'\n",
    "host = 'www.sec.gov'\n",
    "SEC_headers={'User-Agent': SEC_user_agent,'Accept-Encoding':'gzip','Host':host}\n",
    "CIK = '320193'\n",
    "years = range(2000,2023)\n",
    "params = {'action':'getcompany',\n",
    "          'CIK': '789019',\n",
    "          'type': '10-k', #optional\n",
    "          'dateb': '20190101', #optional\n",
    "          #'datea': 20220101, #optional\n",
    "          'owner': 'exclude', # default set to exclude\n",
    "          'start':'',\n",
    "          'output': 'atom',\n",
    "          'count': '100' #number of results I want to see default is 40\n",
    "          } #### Never worked :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticker and CIK mapping\n",
    "tickers_cik = requests.get(\"https://www.sec.gov/files/company_tickers.json\",headers=SEC_headers)\n",
    "tickr_text = tickers_cik.text\n",
    "tickr_dict = json.loads(tickr_text)\n",
    "\n",
    "for num in range(len(tickr_dict)):\n",
    "    tickr_dict[(tickr_dict[str(num)][\"ticker\"])] = tickr_dict[str(num)]\n",
    "    del tickr_dict[str(num)]\n",
    "# for tickr in tickr_dict:\n",
    "#     if tickr_dict[tickr]['cik_str'] == '1265107':\n",
    "#         print(tickr)\n",
    "# POTENTIAL MISSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index for file locations of 10-k and 10-q filings\n",
    "base_url = 'https://www.sec.gov/Archives/edgar/full-index/'\n",
    "quarters = ['QTR1','QTR2','QTR3','QTR4']\n",
    "years = range(2000,2022) \n",
    "for year in years:\n",
    "    try:\n",
    "        os.mkdir(f'./SEC/Master_Index/{year}')\n",
    "    except:\n",
    "        print(f'Previous data from year {year} already saved')\n",
    "    for q in quarters:\n",
    "        print(f'Going through files from {year} {q}...')\n",
    "        master_index = requests.get(base_url+f'{year}/{q}/master.idx', headers=SEC_headers).text\n",
    "        master_index = master_index.split('--------------------------------------------------------------------------------')[1]\n",
    "        master_index = master_index.replace('\\n','|').split('|')\n",
    "        del master_index[0]\n",
    "        keys = master_index[::5]\n",
    "        del master_index[::5]\n",
    "\n",
    "        # Fit data into dictionary and select only 10-k and 10-q file types\n",
    "        shape = int(len(master_index)/4)\n",
    "        print(f'File size for {year} {q}: {shape}')\n",
    "        \n",
    "        data = np.array(master_index).reshape(shape,4)\n",
    "        cols = ['Company Name', 'Form Type', 'Date Filled', 'File Name']\n",
    "        dict_master = {}\n",
    "        for set,key in zip(data,keys):\n",
    "            dict_temp = {key:value for (key,value) in zip(cols,set)}\n",
    "            if dict_temp['Form Type'] == '10-K':\n",
    "                dict_master[key+'_10-k'] = dict_temp\n",
    "            if dict_temp['Form Type'] == '10-Q':\n",
    "                dict_master[key+'_10-q'] = dict_temp\n",
    "        print(f'10-K or 10-Q Filings saved: {len(dict_master)}')\n",
    "        \n",
    "        with open(f'./SEC/Master_Index/{year}/{q}.txt','w') as f:\n",
    "            f.write(json.dumps(dict_master))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.sec.gov/Archives/'\n",
    "# stock = stocks[0]\n",
    "file_types = ['_10-q','_10-k']\n",
    "quarters = ['QTR1','QTR2','QTR3','QTR4']\n",
    "years = ['2020','2021','2022']\n",
    "curr_dir = os.getcwd()\n",
    "folder = 'SEC'\n",
    "# ocf = ['Net cash provided by operating activities', 'Cash generated by operating activities']\n",
    "# title = 'CONSOLIDATED STATEMENTS OF CASH FLOWS'\n",
    "# oex = ['Purchases of property and equipment', 'Payments for acquisition of property, plant and equipment']\n",
    "# for year in years:\n",
    "#     for quarter in quarters:\n",
    "year = '2020'\n",
    "quarter = 'QTR1'\n",
    "file_type = '_10-q'\n",
    "with open(f'./SEC/Master_Index/{year}/{quarter}.txt','r') as f: # Apply different years and quarters\n",
    "    text = f.read()\n",
    "dict_ind = json.loads(text)\n",
    "stocks = 'GOOGL'\n",
    "for stock in stocks:\n",
    "    cik = tickr_dict[stock]['cik_str']\n",
    "    try:\n",
    "        file_type = file_types[0] # Quarterly Filings\n",
    "        url = dict_ind[str(cik)+file_type]['File Name'].replace('-','').replace('.txt','')\n",
    "    except:\n",
    "        file_type = file_types[1] # Yearly Filings\n",
    "        url = dict_ind[str(cik)+file_type]['File Name'].replace('-','').replace('.txt','')\n",
    "    file = '/FilingSummary.xml'\n",
    "    data = requests.get(base_url+url+file,headers=SEC_headers).content\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    myreports = soup.find('myreports')\n",
    "\n",
    "    # List with individual components from myreports\n",
    "    master_reports = []\n",
    "    print(f'Gathering data for {stock}{file_type} @ {quarter} {year}')\n",
    "    for report in myreports.find_all('report')[:-1]:\n",
    "\n",
    "    # dictionary with all relevant parts\n",
    "        report_dict = {}\n",
    "        report_dict['name_short'] = report.shortname.text\n",
    "        report_dict['name_long'] = report.longname.text\n",
    "        report_dict['position'] = report.position.text\n",
    "        report_dict['category'] = report.menucategory.text\n",
    "        report_dict['url'] = base_url + url + '/' + report.htmlfilename.text\n",
    "        master_reports.append(report_dict)\n",
    "        print(report_dict['name_short'])\n",
    "\n",
    "    try: \n",
    "        os.mkdir(os.path.join(curr_dir,folder,stock))\n",
    "    except:\n",
    "        print('Saving master report along existing data {}'.format(stock))\n",
    "    with open(os.path.join(curr_dir,folder,stock,f'master_reports_{file_type}_{year}_{quarter}.json'), 'w') as fout:\n",
    "        json.dump(master_reports , fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_statements_mapping = {\n",
    "    \"Consolidated Balance Sheets\": [\n",
    "        \"Consolidated Balance Sheets\",\n",
    "        \"CONSOLIDATED BALANCE SHEETS\",\n",
    "        \"Condensed Consolidated Balance Sheets\",\n",
    "        \"CONDENSED CONSOLIDATED BALANCE SHEETS\",\n",
    "        \"Consolidated Balance Sheets (Unaudited)\",\n",
    "        \"CONSOLIDATED BALANCE SHEETS (Unaudited)\",\n",
    "        \"Condensed Consolidated Balance Sheets (Unaudited)\",\n",
    "        \"CONDENSED CONSOLIDATED BALANCE SHEETS (Unaudited)\",\n",
    "        \"Balance Sheet\",\n",
    "        \"BALANCE SHEET\",\n",
    "        \"Statement of Financial Position\",\n",
    "        \"STATEMENT OF FINANCIAL POSITION\",\n",
    "        \"Consolidated Statements of Financial Position\",\n",
    "        \"CONSOLIDATED STATEMENTS OF FINANCIAL POSITION\",\n",
    "        \"Consolidated Statements of Financial Condition\",\n",
    "        \"CONSOLIDATED STATEMENTS OF FINANCIAL CONDITION\",\n",
    "        \"Consolidated Statements of Position\",\n",
    "        \"CONSOLIDATED STATEMENTS OF POSITION\",\n",
    "        \"Financial Position\",\n",
    "        \"FINANCIAL POSITION\"\n",
    "    ],\n",
    "    \"Consolidated Statements of Operations\": [\n",
    "        \"Consolidated Statements of Operations\",\n",
    "        \"CONSOLIDATED STATEMENTS OF OPERATIONS\",\n",
    "        \"Consolidated Statements Of Operations\",\n",
    "        \"Consolidated Statements of Operations (Unaudited)\",\n",
    "        \"CONSOLIDATED STATEMENTS OF OPERATIONS (Unaudited)\",\n",
    "        \"Condensed Consolidated Statements of Operations (Unaudited)\",\n",
    "        \"CONDENSED CONSOLIDATED STATEMENTS OF OPERATIONS (Unaudited)\",\n",
    "        \"Consolidated Statements of Income\",\n",
    "        \"CONSOLIDATED STATEMENTS OF INCOME\",\n",
    "        \"Condensed Consolidated Statements of Income\",\n",
    "        \"CONDENSED CONSOLIDATED STATEMENTS OF INCOME\",\n",
    "        \"Income Statement\",\n",
    "        \"INCOME STATEMENT\",\n",
    "        \"Statement of Earnings\",\n",
    "        \"STATEMENT OF EARNINGS\",\n",
    "        \"Profit and Loss Statement (P&L)\",\n",
    "        \"PROFIT AND LOSS STATEMENT (P&L)\",\n",
    "        \"Statement of Comprehensive Income\",\n",
    "        \"STATEMENT OF COMPREHENSIVE INCOME\",\n",
    "        \"Statement of Operations\",\n",
    "        \"STATEMENT OF OPERATIONS\",\n",
    "        \"Statement of Income\",\n",
    "        \"STATEMENT OF INCOME\",\n",
    "        \"Earnings Statement\",\n",
    "        \"EARNINGS STATEMENT\",\n",
    "        \"Revenue and Expense Statement\",\n",
    "        \"REVENUE AND EXPENSE STATEMENT\",\n",
    "        \"Operating Statement\",\n",
    "        \"OPERATING STATEMENT\",\n",
    "        \"Statement of Profit and Loss and Other Comprehensive Income\",\n",
    "        \"STATEMENT OF PROFIT AND LOSS AND OTHER COMPREHENSIVE INCOME\",\n",
    "        \"Statement of Earnings and Retained Earnings\",\n",
    "        \"STATEMENT OF EARNINGS AND RETAINED EARNINGS\",\n",
    "        \"Statement of Income and Expenditure\",\n",
    "        \"STATEMENT OF INCOME AND EXPENDITURE\"\n",
    "    ],\n",
    "    \"Consolidated Statements of Cash Flows\": [\n",
    "        \"Consolidated Statements of Cash Flows\",\n",
    "        \"CONSOLIDATED STATEMENTS OF CASH FLOWS\",\n",
    "        \"Consolidated Statements of Cash Flows (Unaudited)\",\n",
    "        \"CONDENSED CONSOLIDATED STATEMENTS OF CASH FLOWS (Unaudited)\",\n",
    "        \"Condensed Consolidated Statements of Cash Flows\",\n",
    "        \"CONDENSED CONSOLIDATED STATEMENTS OF CASH FLOWS\",\n",
    "        \"Cash Flow Statement\",\n",
    "        \"CASH FLOW STATEMENT\",\n",
    "        \"Statement of Cash Flows\",\n",
    "        \"STATEMENT OF CASH FLOWS\",\n",
    "        \"Statement of Changes in Cash\",\n",
    "        \"STATEMENT OF CHANGES IN CASH\",\n",
    "        \"Statement of Cash Flow\",\n",
    "        \"STATEMENT OF CASH FLOW\"\n",
    "    ],\n",
    "    \"Consolidated Statements of Shareholders' Equity\": [\n",
    "        \"Consolidated Statements of Shareholders' Equity\",\n",
    "        \"CONSOLIDATED STATEMENTS OF SHAREHOLDERS' EQUITY\",\n",
    "        \"Condensed Consolidated Statements of Shareholders' Equity (Unaudited)\",\n",
    "        \"CONDENSED CONSOLIDATED STATEMENTS OF SHAREHOLDERS' EQUITY (Unaudited)\",\n",
    "        \"Consolidated Statements of Redeemable Noncontrolling Interest and Stockholders' Equity (Unaudited)\",\n",
    "        \"CONSOLIDATED STATEMENTS OF REDEEMABLE NONCONTROLLING INTEREST AND STOCKHOLDERS' EQUITY (Unaudited)\",\n",
    "        \"Consolidated Statements of Redeemable Noncontrolling Interest and Stockholders' Equity\",\n",
    "        \"CONSOLIDATED STATEMENTS OF REDEEMABLE NONCONTROLLING INTEREST AND STOCKHOLDERS' EQUITY\",\n",
    "        \"Consolidated Statements of Stockholders' Equity\",\n",
    "        \"CONSOLIDATED STATEMENTS OF STOCKHOLDERS' EQUITY\",\n",
    "        \"Condensed Statements of Shareholders' Equity\",\n",
    "        \"CONDENSED STATEMENTS OF SHAREHOLDERS' EQUITY\",\n",
    "        \"Statement of Shareholders' Equity\",\n",
    "        \"STATEMENT OF SHAREHOLDERS' EQUITY\",\n",
    "        \"Shareholders' Equity Statement\",\n",
    "        \"STOCKHOLDERS' EQUITY\",\n",
    "        \"Stockholders' Equity\",\n",
    "        \"SHAREHOLDERS' EQUITY STATEMENT\",\n",
    "        \"Equity Statement\",\n",
    "        \"EQUITY STATEMENT\"\n",
    "        \n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earnings files are collected with desired data\n",
    "### Figure out all possible formatting for the document names ###\n",
    "stocks = ['GOOGL','AAPL','TSLA','AMZN','META', 'SNAP']\n",
    "file_types = ['_10-q','_10-k']\n",
    "quarters = ['QTR1','QTR2','QTR3','QTR4']\n",
    "years = ['2020','2021','2022']\n",
    "curr_dir = os.getcwd()\n",
    "folder = 'SEC'\n",
    "statements_url = []\n",
    "# item1 = [r\"Consolidated Balance Sheets\", r\"CONSOLIDATED BALANCE SHEETS\", r\"CONDENSED CONSOLIDATED BALANCE SHEETS (Unaudited)\"]   #\"CONSOLIDATED BALANCE SHEETS\"\n",
    "# item2 = [r\"Consolidated Statements of Operations\", r\"CONDENSED CONSOLIDATED STATEMENTS OF OPERATIONS (Unaudited)\", r\"CONSOLIDATED STATEMENTS OF INCOME\", r\"Consolidated Statements Of Operations (Unaudited)\", r\"CONSOLIDATED STATEMENTS OF OPERATIONS\"]   #\"CONDENSED CONSOLIDATED STATEMENTS OF OPERATIONS (Unaudited)\"\n",
    "# item3 = [r\"Consolidated Statements of Cash Flows (Unaudited)\", r\"CONSOLIDATED STATEMENTS OF CASH FLOWS\", r\"CONDENSED CONSOLIDATED STATEMENTS OF CASH FLOWS (Unaudited)\", r\"Consolidated Statements of Cash Flows\"] #\"CONSOLIDATED STATEMENTS OF CASH FLOWS\" #CONDENSED CONSOLIDATED STATEMENTS OF CASH FLOWS (Unaudited)\"\n",
    "# item4 = [r\"Consolidated Statements of Redeemable Noncontrolling Interest and Stockholders' Equity\", r\"CONDENSED CONSOLIDATED STATEMENTS OF SHAREHOLDERS' EQUITY (Unaudited)\", r\"CONSOLIDATED STATEMENTS OF STOCKHOLDERS' EQUITY\", r\"Consolidated Statements of Stockholders' Equity\", r\"CONSOLIDATED STATEMENTS OF SHAREHOLDERS' EQUITY\"]   #\"CONDENSED CONSOLIDATED STATEMENTS OF SHAREHOLDERS' EQUITY (Unaudited)\"\n",
    "   \n",
    "missing_data_ls = [] \n",
    "num_missing = 0\n",
    "\n",
    "for stock in stocks:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(curr_dir,folder,stock,'financial_statements_url'))\n",
    "    except:\n",
    "        print('...previous financial statements already exist...')\n",
    "    for year in years:\n",
    "        for quarter in quarters:\n",
    "            print('*'*100)\n",
    "            print(f'{stock} {year} {quarter}')\n",
    "            try:\n",
    "                file_type = file_types[0]\n",
    "                with open(os.path.join(curr_dir,folder,stock,'master_reports',f'master_reports_{file_type}_{year}_{quarter}.json')) as f:\n",
    "                    master_reports = json.load(f)\n",
    "            except:\n",
    "                file_type = file_types[1]\n",
    "                with open(os.path.join(curr_dir,folder,stock,'master_reports',f'master_reports_{file_type}_{year}_{quarter}.json')) as f:\n",
    "                    master_reports = json.load(f)\n",
    "\n",
    "            missing_data = [item for item in financial_statements_mapping]\n",
    "            # THIS NEEDS TO BE DONE WITH RE ### this text may change slightly per quarter\n",
    "            statements_url = []\n",
    "            dict_url = {}\n",
    "            for report_dict in master_reports:\n",
    "                \n",
    "                report_list = financial_statements_mapping \n",
    "                for item in report_list:\n",
    "                    if report_dict['name_short'] in financial_statements_mapping[item]:                        \n",
    "\n",
    "                        print('-'*100)\n",
    "                        print(report_dict['name_short'])\n",
    "                        print(report_dict['url'])\n",
    "                        \n",
    "                        \n",
    "                        # print(item)\n",
    "                        # print(report_dict['name_short'])\n",
    "                        # print(missing_data)\n",
    "                        try:\n",
    "                            missing_data.remove(item)\n",
    "                            statements_url.append(report_dict['url'])\n",
    "                            dict_url[item] = report_dict['url']\n",
    "                        except ValueError:\n",
    "                            print(f'MULTIPLE REPORTS FOUND FOR: {item}')\n",
    "            \n",
    "            with open(os.path.join(curr_dir,folder,stock,'financial_statements_url',f'url{file_type}_{year}_{quarter}.json'), 'w') as f:\n",
    "                json.dump(dict_url,f)\n",
    "                \n",
    "                        \n",
    "\n",
    "                    # elif report_dict['name_short'] in ['CONDENSED ' + x for x in financial_statements_mapping[item]]:\n",
    "                    \n",
    "                    #     print('-'*100)\n",
    "                    #     print(report_dict['name_short'])\n",
    "                    #     print(report_dict['url'])\n",
    "                        \n",
    "                    #     statements_url.append(report_dict['url'])\n",
    "                    #     missing_data.remove(item)\n",
    "            if len(missing_data) != 0:\n",
    "                num_missing += len(missing_data)\n",
    "                missing_data_ls.append(f'Missing Data for {stock}, {year}, {quarter}, {file_type} \\n {len(missing_data)}')\n",
    "                \n",
    "                \n",
    "for x in missing_data_ls:\n",
    "    print(x)\n",
    "    \n",
    "print(f'Total: {num_missing}')\n",
    "print(statements_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./SEC/AMZN/financial_statements_url/url_10-q_2020_QTR2.json','r') as f: \n",
    "    text = f.read()\n",
    "    statements_url = json.loads(text)\n",
    "print(statements_url)\n",
    "\n",
    "with open(f'./SEC/GOOGL/financial_statements_url/url_10-q_2020_QTR2.json','r') as f: \n",
    "    text = f.read()\n",
    "    statements_url = json.loads(text)\n",
    "print(statements_url)\n",
    "\n",
    "with open(f'./SEC/TSLA/financial_statements_url/url_10-q_2020_QTR2.json','r') as f: \n",
    "    text = f.read()\n",
    "    statements_url = json.loads(text)\n",
    "print(statements_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume we want all the statements in a single data set.\n",
    "statements_data = []\n",
    "\n",
    "# loop through each statement url\n",
    "for statement in statements_url.values():\n",
    "\n",
    "    # define a dictionary that will store the different parts of the statement.\n",
    "    statement_data = {}\n",
    "    statement_data['headers'] = []\n",
    "    statement_data['sections'] = []\n",
    "    statement_data['data'] = []\n",
    "    \n",
    "    # request the statement file content\n",
    "    content = requests.get(statement, headers=SEC_headers).content\n",
    "    report_soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "    # find all the rows, figure out what type of row it is, parse the elements, and store in the statement file list.\n",
    "    for index, row in enumerate(report_soup.table.find_all('tr')):\n",
    "        \n",
    "        # first let's get all the elements.\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        # if it's a regular row and not a section or a table header\n",
    "        if (len(row.find_all('th')) == 0 and len(row.find_all('strong')) == 0): \n",
    "            reg_row = [ele.text.strip() for ele in cols]\n",
    "            statement_data['data'].append(reg_row)\n",
    "            \n",
    "        # if it's a regular row and a section but not a table header\n",
    "        elif (len(row.find_all('th')) == 0 and len(row.find_all('strong')) != 0):\n",
    "            sec_row = cols[0].text.strip()\n",
    "            statement_data['sections'].append(sec_row)\n",
    "            \n",
    "        # finally if it's not any of those it must be a header\n",
    "        elif (len(row.find_all('th')) != 0):            \n",
    "            hed_row = [ele.text.strip() for ele in row.find_all('th')]\n",
    "            statement_data['headers'].append(hed_row)\n",
    "            \n",
    "        else:            \n",
    "            print('We encountered an error.')\n",
    "\n",
    "    # append it to the master list.\n",
    "    statements_data.append(statement_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_data[2]['headers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = statements_data[0]['headers'][0]\n",
    "[ item for item in l for _ in range(2) ]\n",
    "n = 2\n",
    "file_type = '_10-q'\n",
    "period_mapping = {'_10-q' : '3 Months Ended', '_10-k' : '12 Months Ended'}\n",
    "\n",
    "if len(statements_data[n]['headers']) != 1:\n",
    "    subheaders = statements_data[n]['headers'][1]\n",
    "    overheaders = statements_data[n]['headers'][0][1:]\n",
    "    mult = int(len(subheaders)/len(overheaders))\n",
    "    overheaders_table = [item for item in overheaders for _ in range(mult)]\n",
    "    income_header = [x+' ('+y+')' for x,y in zip(subheaders,overheaders_table)]\n",
    "else:\n",
    "    subheaders = statements_data[n]['headers'][0][1:]\n",
    "    income_header = [x+' ('+period_mapping[file_type]+')' for x in subheaders]\n",
    "\n",
    "print(income_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "folder = 'SEC'\n",
    "data_dir = os.path.join(curr_dir,folder,'AAPL','financial_statements_raw')\n",
    "for data in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir,data)) as f:\n",
    "        statements_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_data[2]['headers'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_data[0]['headers'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the proper components\n",
    "i = 3\n",
    "### Figure out all possible formatting according to document title, number of headers, and display table accordingly ###\n",
    "\n",
    "if len(statements_data[i]['headers']) != 1:\n",
    "    subheaders = statements_data[i]['headers'][1]\n",
    "    overheaders = statements_data[i]['headers'][0][1:]\n",
    "    mult = int(len(subheaders)/len(overheaders))\n",
    "    overheaders_table = [item for item in overheaders for _ in range(mult)]\n",
    "    income_header = [x+' ('+y+')' for x,y in zip(subheaders,overheaders_table)]\n",
    "else:\n",
    "    subheaders = statements_data[i]['headers'][0][1:]\n",
    "    income_header = [x+' ('+period_mapping[file_type]+')' for x in subheaders]\n",
    "\n",
    "# income_header =  statements_data[i]['headers'][-1] #threee months ended gives different formatting #[0][1:]\n",
    "income_data = statements_data[i]['data']\n",
    "\n",
    "# Put the data in a DataFrame\n",
    "income_df = pd.DataFrame(income_data)\n",
    "\n",
    "# Display\n",
    "print('-'*100)\n",
    "print('Before Reindexing')\n",
    "print('-'*100)\n",
    "display(income_df.head())\n",
    "\n",
    "# Define the Index column, rename it, and we need to make sure to drop the old column once we reindex.\n",
    "income_df.index = income_df[0]\n",
    "income_df.index.name = 'Category'\n",
    "income_df = income_df.drop(0, axis = 1)\n",
    "\n",
    "# Display\n",
    "print('-'*100)\n",
    "print('Before Regex')\n",
    "print('-'*100)\n",
    "display(income_df.head())\n",
    "\n",
    "# Get rid of the '$', '(', ')', and convert the '' to NaNs.\n",
    "income_df = income_df.replace('[\\$,)]','', regex=True )\\\n",
    "                     .replace( '[(]','-', regex=True)\\\n",
    "                     .replace( '', 'NaN', regex=True)\n",
    "\n",
    "# Display\n",
    "print('-'*100)\n",
    "print('Before type conversion')\n",
    "print('-'*100)\n",
    "display(income_df.head())\n",
    "\n",
    "# everything is a string, so let's convert all the data to a float.\n",
    "df_copy = income_df.copy()\n",
    "income_df = income_df.astype(float)\n",
    "\n",
    "# Change the column headers\n",
    "print(income_header)\n",
    "income_df.columns = income_header\n",
    "\n",
    "# Display\n",
    "print('-'*100)\n",
    "print('Final Product')\n",
    "print('-'*100)\n",
    "\n",
    "# show the df\n",
    "display(income_df)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "try:\n",
    "    folder_dir = os.path.join(current_dir,'SEC',stock)\n",
    "    os.mkdir(folder_dir)\n",
    "except:\n",
    "    print(f'Saving over data for {stock} in {year} {quarter}...')\n",
    "income_df.to_csv(os.path.join(folder_dir,f\"{year+quarter+file_type}.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the proper components\n",
    "\n",
    "### Figure out all possible formatting according to document title, number of headers, and display table accordingly ###\n",
    "income_header =  statements_data[1]['headers'][1] #threee months ended gives different formatting #[0][1:]\n",
    "income_data = statements_data[1]['data']\n",
    "\n",
    "# Put the data in a DataFrame\n",
    "income_df = pd.DataFrame(income_data)\n",
    "\n",
    "# Define the Index column, rename it, and we need to make sure to drop the old column once we reindex.\n",
    "income_df.index = income_df[0]\n",
    "income_df.index.name = 'Category'\n",
    "income_df = income_df.drop(0, axis = 1)\n",
    "\n",
    "# Get rid of the '$', '(', ')', and convert the '' to NaNs.\n",
    "income_df = income_df.replace('[\\$,)]','', regex=True )\\\n",
    "                     .replace( '[(]','-', regex=True)\\\n",
    "                     .replace( '', 'NaN', regex=True)\n",
    "\n",
    "# everything is a string, so let's convert all the data to a float.\n",
    "income_df = income_df.astype(float)\n",
    "\n",
    "# Change the column headers\n",
    "income_df.columns = income_header\n",
    "\n",
    "# show the df\n",
    "display(income_df)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "try:\n",
    "    folder_dir = os.path.join(current_dir,'SEC',stock)\n",
    "    os.mkdir(folder_dir)\n",
    "except:\n",
    "    print(f'Saving over data for {stock} in {year} {quarter}...')\n",
    "income_df.to_csv(os.path.join(folder_dir,f\"{year+quarter+file_type}.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df.to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Filings for a stock\n",
    "base_url = 'https://www.sec.gov/Archives/'\n",
    "ocf = ['Net cash provided by operating activities', 'Cash generated by operating activities']\n",
    "title = 'CONSOLIDATED STATEMENTS OF CASH FLOWS'\n",
    "oex = ['Purchases of property and equipment', 'Payments for acquisition of property, plant and equipment']\n",
    "with open('./SEC/Master_Index/2020/QTR1.txt','r') as f:\n",
    "    text = f.read()\n",
    "dict_ind = json.loads(text)\n",
    "cik = tickr_dict['META']['cik_str']\n",
    "url = dict_ind[str(cik)+'_10-k']['File Name'].replace('-','').replace('.txt','')\n",
    "for x in range(1,90):\n",
    "    file = f'R{x}.htm'\n",
    "    data = requests.get(base_url+url+f'/{file}',headers=SEC_headers).text\n",
    "    if title in data:\n",
    "        print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get(base_url+url+f'/R8.htm',headers=SEC_headers).text\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gather Data\n",
    "def YfinanceData(Symbols,daymonthyear):\n",
    "    # Yfinance_list = []\n",
    "    Yfinance_dict = {}\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "\n",
    "    # Interval required 5 minutes\n",
    "    start = dt.datetime(daymonthyear[2], daymonthyear[1], daymonthyear[0])\n",
    "\n",
    "    for Symbol in Symbols:\n",
    "        Yfinance_DataFrame = yf.download(tickers=Symbol, interval='1d', start= start)\n",
    "        Yfinance_DataFrame['Ticker'] = Symbol\n",
    "        # Yfinance_list.append(Yfinance_DataFrame)\n",
    "        Yfinance_dict[Symbol] = Yfinance_DataFrame\n",
    "        data_dir = os.path.join(current_dir,f'Stock_Data/{Symbol}_{daymonthyear[2]}{daymonthyear[1]}{daymonthyear[0]}.csv')\n",
    "        Yfinance_DataFrame.to_csv(data_dir)\n",
    "        \n",
    "    return Yfinance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StockPerformance(tickers,daymonthyear):\n",
    "    current_dir = os.getcwd()\n",
    "    analysis_dir = os.path.join(current_dir,'Stock_Analysis')\n",
    "    data_dir = os.path.join(current_dir,'Stock_Data')\n",
    "    for tckr in tickers:\n",
    "        data = pd.read_csv(os.path.join(data_dir,f'{tckr}_{daymonthyear[2]}{daymonthyear[1]}{daymonthyear[0]}.csv'))\n",
    "        \n",
    "        ticker_dir = os.path.join(analysis_dir,tckr)\n",
    "        try:\n",
    "            os.mkdir(ticker_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(data['Close'])\n",
    "        plt.title(f'{tckr}_{daymonthyear[2]}{daymonthyear[1]}{daymonthyear[0]}')\n",
    "        plt.savefig(os.path.join(analysis_dir,tckr,f'{tckr}_{daymonthyear[2]}{daymonthyear[1]}{daymonthyear[0]}_plot.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify stocks and Timeperiod\n",
    "tickers = ['AMZN','GOOG','APPL']\n",
    "daymonthyear = [30,12,2005]\n",
    "data = YfinanceData(tickers,daymonthyear)\n",
    "StockPerformance(tickers,daymonthyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_cap_data = web.get_quote_yahoo(tickers)['marketCap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla = yf.Ticker(\"TSLA\")\n",
    "tsla.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(3):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Category': ['Fruit','Vegetable','Vegetable','Fruit','Vegetable','Vegetable','Fruit','Vegetable','Fruit','Vegetable'],\n",
    "    'SubCategories': ['Apple','Brinjal','Brinjal','Apple','Carrot','Potato','Apple','Carrot','Banana','Brinjal'],\n",
    "    'Count': [2,1,1,1,3,1,1,2,1,1],\n",
    "})\n",
    "\n",
    "df.set_index(['Category','SubCategories']).groupby(level=[0,1]).sum()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic ={\"geeks\": \"A\",\"for\":\"B\",\"geeks\":\"C\"}\n",
    "\n",
    "value = {i for i in dic if dic[i]==\"B\"}\n",
    "print(\"key by value:\",value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [60098.0, 49481.0],\n",
       " 'year': ['2020', '2019'],\n",
       " 'quarter': ['QTR3', 'QTR3'],\n",
       " 'months_ended': ['9', '9']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [43827.0, 37845.0],\n",
       " 'year': ['2020', '2019'],\n",
       " 'quarter': ['QTR2', 'QTR2'],\n",
       " 'months_ended': ['6', '6']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [69391.0, 77434.0, 64225.0],\n",
       " 'year': ['2019', '2018', '2017'],\n",
       " 'quarter': ['QTR4', 'QTR4', 'QTR4'],\n",
       " 'months_ended': ['12', '12', '12']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [62744.0, 43827.0],\n",
       " 'year': ['2021', '2020'],\n",
       " 'quarter': ['QTR2', 'QTR2'],\n",
       " 'months_ended': ['6', '6']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [83838.0, 60098.0],\n",
       " 'year': ['2021', '2020'],\n",
       " 'quarter': ['QTR3', 'QTR3'],\n",
       " 'months_ended': ['9', '9']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [26690.0, 28293.0],\n",
       " 'year': ['2018', '2017'],\n",
       " 'quarter': ['QTR1', 'QTR1'],\n",
       " 'months_ended': ['3', '3']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [46966.0, 38763.0],\n",
       " 'year': ['2021', '2020'],\n",
       " 'quarter': ['QTR1', 'QTR1'],\n",
       " 'months_ended': ['3', '3']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [122151.0, 104038.0, 80674.0],\n",
       " 'year': ['2022', '2021', '2020'],\n",
       " 'quarter': ['QTR4', 'QTR4', 'QTR4'],\n",
       " 'months_ended': ['12', '12', '12']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [49481.0, 57911.0],\n",
       " 'year': ['2019', '2018'],\n",
       " 'quarter': ['QTR3', 'QTR3'],\n",
       " 'months_ended': ['9', '9']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [37845.0, 43423.0],\n",
       " 'year': ['2019', '2018'],\n",
       " 'quarter': ['QTR2', 'QTR2'],\n",
       " 'months_ended': ['6', '6']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [38763.0, 30516.0],\n",
       " 'year': ['2020', '2019'],\n",
       " 'quarter': ['QTR1', 'QTR1'],\n",
       " 'months_ended': ['3', '3']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [80674.0, 69391.0, 77434.0],\n",
       " 'year': ['2020', '2019', '2018'],\n",
       " 'quarter': ['QTR4', 'QTR4', 'QTR4'],\n",
       " 'months_ended': ['12', '12', '12']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [30516.0, 26690.0],\n",
       " 'year': ['2019', '2018'],\n",
       " 'quarter': ['QTR1', 'QTR1'],\n",
       " 'months_ended': ['3', '3']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [104038.0, 80674.0, 69391.0],\n",
       " 'year': ['2021', '2020', '2019'],\n",
       " 'quarter': ['QTR4', 'QTR4', 'QTR4'],\n",
       " 'months_ended': ['12', '12', '12']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [98024.0, 83838.0],\n",
       " 'year': ['2022', '2021'],\n",
       " 'quarter': ['QTR3', 'QTR3'],\n",
       " 'months_ended': ['9', '9']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cash_flow': [75132.0, 62744.0],\n",
       " 'year': ['2022', '2021'],\n",
       " 'quarter': ['QTR2', 'QTR2'],\n",
       " 'months_ended': ['6', '6']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting relevances from tabular data\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "\n",
    "##### PARAMETERS ######\n",
    "with open(\"SEC_config.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "SEC_user_agent = cfg['SEC_user_agent']\n",
    "encoding = cfg['encoding']\n",
    "host = cfg['host']\n",
    "SEC_headers = {'User-Agent': SEC_user_agent,'Accept-Encoding':'gzip','Host':host} #cfg['SEC_headers']\n",
    "years = range(cfg['years']['start'],cfg['years']['end'])\n",
    "params = cfg['params'] \n",
    "quarters = cfg['quarters']\n",
    "stocks = ['AAPL']\n",
    "\n",
    "file_types = ['_10-q','_10-k']\n",
    "base_url = 'https://www.sec.gov/Archives/'\n",
    "curr_dir = os.getcwd()\n",
    "folder = 'SEC'\n",
    "\n",
    "##############################\n",
    "def cash_flow_statement_extraction(statement_quarter, df_statement):\n",
    "    filt = df_statement['Category'] == 'Cash generated by operating activities'\n",
    "    df_extract = df_statement.loc[filt]\n",
    "    years = []\n",
    "    months_ended = []\n",
    "    quarters = []\n",
    "    cfs = []\n",
    "\n",
    "    for col in df_extract.columns:\n",
    "        year, month_ended = extract_date_and_months(col)\n",
    "        cf = df_extract[col].values[0]\n",
    "        if (years == None) | (month_ended == None):\n",
    "            continue\n",
    "\n",
    "        years.append(year)\n",
    "        months_ended.append(month_ended)\n",
    "        quarters.append(statement_quarter)\n",
    "        cfs.append(cf)\n",
    "\n",
    "    cf_dict = {'cash_flow': cfs, 'year': years, 'quarter': quarters, 'months_ended': months_ended}\n",
    "    return cf_dict\n",
    "\n",
    "def extract_months(col_name):\n",
    "    try:\n",
    "        match = re.search(r'(\\d+)\\s+Months\\s+Ended', col_name)\n",
    "        return match.group(1) if match else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_date_and_months(col_name):\n",
    "    year_match = re.search(r'\\b\\d{4}\\b', col_name)\n",
    "    months_match = re.search(r'\\b\\d+\\sMonths\\sEnded\\b', col_name)\n",
    "    \n",
    "    year = year_match.group(0) if year_match else None\n",
    "    months = months_match.group(0) if months_match else None\n",
    "    \n",
    "    extracted_months = extract_months(months)\n",
    "\n",
    "    return year, extracted_months\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for stock in stocks:\n",
    "\n",
    "        tab_data_path = os.path.join(curr_dir,'SEC',stock,'tabular_data')\n",
    "        statements = os.listdir(tab_data_path) \n",
    "        df_stock_cf_10k = pd.DataFrame({'cash_flow': [], 'year': [], 'quarter': [], 'months_ended': []})\n",
    "        df_stock_cf_10q = pd.DataFrame({'cash_flow': [], 'year': [], 'quarter': [], 'months_ended': []})\n",
    "\n",
    "        for statement in statements:\n",
    "            \n",
    "            if statement.startswith('cash_flow_statement') & statement.endswith('_10-k.csv'):\n",
    "                df_tab_data = pd.read_csv(os.path.join(tab_data_path,statement))\n",
    "                cf_dict = cash_flow_statement_extraction(statement[-13:-9], df_tab_data)\n",
    "                df_stock_cf_10k = pd.concat([df_stock_cf_10k,pd.DataFrame(cf_dict)])\n",
    "\n",
    "            if statement.startswith('cash_flow_statement'):\n",
    "                df_tab_data = pd.read_csv(os.path.join(tab_data_path,statement))\n",
    "                cf_dict = cash_flow_statement_extraction(statement[-13:-9], df_tab_data)\n",
    "                display(cf_dict)\n",
    "                df_stock_cf_10q = pd.concat([df_stock_cf_10q,pd.DataFrame(cf_dict)])\n",
    "\n",
    "        df_stock_cf_10k = df_stock_cf_10k.drop_duplicates().sort_values(by=['year'])\n",
    "        df_stock_cf_10q = df_stock_cf_10q.drop_duplicates().sort_values(by=['year','quarter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quarterly_cash_flow(df):\n",
    "    df = df.sort_values(by=['year', 'quarter'])\n",
    "    df['quarterly_cash_flow'] = 0.0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'months_ended'] == 3:\n",
    "            df.at[i, 'quarterly_cash_flow'] = df.at[i, 'cash_flow']\n",
    "        elif df.at[i, 'months_ended'] == 6:\n",
    "            if i >= 1 and df.at[i-1, 'months_ended'] == 3:\n",
    "                df.at[i, 'quarterly_cash_flow'] = df.at[i, 'cash_flow'] - df.at[i-1, 'cash_flow']\n",
    "        elif df.at[i, 'months_ended'] == 9:\n",
    "            if i >= 2 and df.at[i-2, 'months_ended'] == 3 and df.at[i-1, 'months_ended'] == 6:\n",
    "                df.at[i, 'quarterly_cash_flow'] = df.at[i, 'cash_flow'] - df.at[i-1, 'cash_flow'] - df.at[i-2, 'cash_flow']\n",
    "        elif df.at[i, 'months_ended'] == 12:\n",
    "            if i >= 3 and df.at[i-3, 'months_ended'] == 3 and df.at[i-2, 'months_ended'] == 6 and df.at[i-1, 'months_ended'] == 9:\n",
    "                df.at[i, 'quarterly_cash_flow'] = df.at[i, 'cash_flow'] - df.at[i-1, 'cash_flow'] - df.at[i-2, 'cash_flow'] - df.at[i-3, 'cash_flow']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    False\n",
       "2    False\n",
       "Name: months_ended, dtype: bool"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock_cf_10q.at[i, 'months_ended'] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/td/24bjbjdd2fdfcpk1wrnr3f4c0000gn/T/ipykernel_45889/3624918346.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_quarterly_cash_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stock_cf_10q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/td/24bjbjdd2fdfcpk1wrnr3f4c0000gn/T/ipykernel_45889/2323709688.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quarter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quarterly_cash_flow'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'months_ended'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quarterly_cash_flow'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cash_flow'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'months_ended'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'months_ended'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Repos/Webscraper/Webscraper_env/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "filt = \n",
    "calculate_quarterly_cash_flow(df_stock_cf_10q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cash_flow</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>months_ended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28293.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>QTR1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64225.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>QTR4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26690.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>QTR1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43423.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>QTR2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57911.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>QTR3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77434.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>QTR4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30516.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>QTR1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37845.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>QTR2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49481.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>QTR3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69391.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>QTR4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38763.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>QTR1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43827.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>QTR2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60098.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>QTR3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80674.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>QTR4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46966.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>QTR1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62744.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>QTR2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83838.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>QTR3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104038.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>QTR4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75132.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>QTR2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98024.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>QTR3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122151.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>QTR4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cash_flow  year quarter months_ended\n",
       "1    28293.0  2017    QTR1            3\n",
       "2    64225.0  2017    QTR4           12\n",
       "0    26690.0  2018    QTR1            3\n",
       "1    43423.0  2018    QTR2            6\n",
       "1    57911.0  2018    QTR3            9\n",
       "1    77434.0  2018    QTR4           12\n",
       "1    30516.0  2019    QTR1            3\n",
       "1    37845.0  2019    QTR2            6\n",
       "1    49481.0  2019    QTR3            9\n",
       "0    69391.0  2019    QTR4           12\n",
       "1    38763.0  2020    QTR1            3\n",
       "0    43827.0  2020    QTR2            6\n",
       "0    60098.0  2020    QTR3            9\n",
       "2    80674.0  2020    QTR4           12\n",
       "0    46966.0  2021    QTR1            3\n",
       "0    62744.0  2021    QTR2            6\n",
       "0    83838.0  2021    QTR3            9\n",
       "1   104038.0  2021    QTR4           12\n",
       "0    75132.0  2022    QTR2            6\n",
       "0    98024.0  2022    QTR3            9\n",
       "0   122151.0  2022    QTR4           12"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock_cf_10q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package             Version\n",
      "------------------- -----------\n",
      "appnope             0.1.4\n",
      "asttokens           2.4.1\n",
      "beautifulsoup4      4.12.3\n",
      "certifi             2024.2.2\n",
      "charset-normalizer  3.3.2\n",
      "comm                0.2.2\n",
      "contourpy           1.2.1\n",
      "cycler              0.12.1\n",
      "debugpy             1.8.1\n",
      "decorator           5.1.1\n",
      "exceptiongroup      1.2.1\n",
      "executing           2.0.1\n",
      "fonttools           4.52.4\n",
      "idna                3.7\n",
      "importlib_metadata  7.1.0\n",
      "importlib_resources 6.4.0\n",
      "ipykernel           6.29.4\n",
      "ipython             8.18.1\n",
      "jedi                0.19.1\n",
      "jsons               1.6.3\n",
      "jupyter_client      8.6.2\n",
      "jupyter_core        5.7.2\n",
      "kiwisolver          1.4.5\n",
      "lxml                5.2.2\n",
      "matplotlib          3.9.0\n",
      "matplotlib-inline   0.1.7\n",
      "nest-asyncio        1.6.0\n",
      "numpy               1.26.4\n",
      "packaging           24.0\n",
      "pandas              2.2.2\n",
      "pandas-datareader   0.10.0\n",
      "parso               0.8.4\n",
      "pexpect             4.9.0\n",
      "pillow              10.3.0\n",
      "pip                 21.2.3\n",
      "platformdirs        4.2.2\n",
      "prompt_toolkit      3.0.45\n",
      "psutil              5.9.8\n",
      "ptyprocess          0.7.0\n",
      "pure-eval           0.2.2\n",
      "Pygments            2.18.0\n",
      "pyparsing           3.1.2\n",
      "python-dateutil     2.9.0.post0\n",
      "pytz                2024.1\n",
      "PyYAML              6.0.1\n",
      "pyzmq               26.0.3\n",
      "requests            2.32.2\n",
      "setuptools          57.4.0\n",
      "six                 1.16.0\n",
      "soupsieve           2.5\n",
      "stack-data          0.6.3\n",
      "tornado             6.4\n",
      "traitlets           5.14.3\n",
      "typing_extensions   4.12.0\n",
      "typish              1.9.3\n",
      "tzdata              2024.1\n",
      "urllib3             2.2.1\n",
      "wcwidth             0.2.13\n",
      "zipp                3.19.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/matiashuber/Desktop/Repos/Webscraper/Webscraper_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4434cb61a64a29bdc4ba835d1ab4bdbe0d581e6a27c99b519cc3fa150616e002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
